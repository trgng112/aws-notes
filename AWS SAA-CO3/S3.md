- Main building blocks of AWS
- advertised as "infinite scaling" storage
- Many websites use S3 as a backbone
- Many AWS services use S3 as an integration as well

### Use cases
- Backup and storage
- Disaster recovery
- Archive
- Hybrid cloud storage
- Application hosting
- media hosting
- data lakes & big data analytics
- software delivery
- static website
- ![[Pasted image 20250726115747.png]]

### Buckets
- S3 allows people to store objects (files) in "buckets" (directories)
- buckets must have a globally unique name (across all regions all accounts)
- buckets are dined at the region levle
- **S3 looks like a global service but buckets are created in a region**
- Naming convention
	- No uppercase, no underscore
	- 3-63 characters long
	- Not an IP
	- Must start with lowercase letter or number
	- Must Not start with the prefix xn--
	- Must not end with the suffix -s3alias

### Objects
- Objects (files) have a Key
- the Key is the FULL path:
	- s3://my-bucket/**my_file.txt**
	- s3://my-bucket/**my_folder1/anothger_folder/my_file.txt**
- The key is composed of prefix + object name
	- s3://my-bucket/**my_folder1/another_folder/my_file.txt**
-  theres no concept of "directories" within buckets (although the UI will trick you to think otherwise)
- just keys with very long name that contains slashes
- Object values are the content of the body:
	- Max object size is 5TB
	- If uploading more than 5gb, must use "multi-part upload"
- Metadata (list of text key/value pairs - system or user metadata)
- Tags (unicode key/ value pair -up to 10) - useful for security / lifecycle
- Version ID (if versioning is enabled)


### Security
- User based
	- IAM policies - which API calls should be allowed for a specific user from IAM
- Resource-based
	- Bucket policies - bucket wide rules from the S3 console - allows cross account
	- Object Access Control LIst (ACL) - finer grain (can be disabled)
	- Bucket Access Control List (ACL) - less common (can be disabled)
- **Note: an IAM principal can access an S3 object if**
	- **The user IAM permissions ALLOW it OR the resource policy ALLOWS it**
	- **And There's no explicit DENY**
- Encryption: encrypt objects in Amazon S# using encryption keys

#### S3 Bucket Policies
- Json based policies
	- Resources: buckets and objects
	- Effect: Allow/ Deny
	- Actions: Set of API to Allow or Deny
	- Principal: The account or user to apply the policy to
- Use S3 bucket for policy to:
	- Grant public access to the bucket
	- Force objects to be encrypted at upload
	- Grant access to another account (Cross Account)
	- ![[Pasted image 20250726122705.png]]
- Example: 

	- ![[Pasted image 20250726122821.png]]
	- ![[Pasted image 20250726122831.png]]
	- ![[Pasted image 20250726122811.png]]
	- ![[Pasted image 20250726122910.png]]

### Static website hosting
- S3 can host static websites and have the accessible on the Internet 
- Website URL will be (depending on the region)
	- http://**bucket-name**.s3-websites-**aws-region**.amazonAWS.com
-  iF YOU GET 403 MAKE SURE THE BUKCET POLICY ALLOWS PUBLIC READS

### Versioning
- You can version your files in the S3
- enabled at the bucket level
- Same key overwrite will change the version: 1,2,3
- it is best practice to version your buckets
	- Protect against unintended deletes (restore a version)
	- easy rollback to previous version
- Notes:
	- Any file that is not versioned prior to enabling versioning will have version "null"
	- Suspending versioning does not delete the previous versions
### Replication (CRR & SRR)
- Must enable Versioning in source and destination buckets
- Cross- region replication (CRR)
- Same-Region Replication (SRR)
- Buckets can be different AWS accounts
- Copying is asynchronous
- Must give proper IAM permissions to S3
- Use cases:
	- CRR - compliance, lower latency access, replication across accounts
	- SRR - log aggregation, live replication between production and test accounts
- Notes:
	- After you enable replication, only new objects are replicated
	- optionally, you can replicate existing objects using **S3 Batch Replication**
		- Replicates existing objects and objects that failed replication
	- For DELETE operations
		- **Can replicate delete markers** from source to target (optional settings)
		- Deletions with a version ID are not replicated (to avoid malicious deletes)
	- **There is no chaining of replication** 
		- If bucket 1 has replication into bucket 2, which has replication into bucket 3
		- Then objects created in bucket 1 are not replicated to bucket 3

### S3 Storage Classes
- S3 Standard - General purpose
- Standard - Infrequent access (IA)
- One ZOne - Infrequent ACcess
- Glacier Instant Retrieval
- Glacier Flexible Retrieval
- Glacier Deep Archive
- Intelligent tiering
- Can move between classes manually or using S3 lifecycle configurations

#### Durability and availability
- Durability:
	- High durability (99.99999999999%, 11 9's) of objects across multiple AZ
	- if you store 10,000,000 objects with S3, you can on average expect to incur a loss of a single object once every 10,000 years
	- Same for all storage classes
- Availability:
	- Measures how readily available a service is
	- Varies depending on storage class
	- example: s3 standard has 99.99% availability = not available 53 minutes a year

#### S3 Standard - General purpose
- 99.99 availability
- used for frequently accessed data
- low latency and high throughput
- sustain 2 concurrent facility failures
- use cases: big data analytics, mobile, gaming applications, content distribution

#### Infrequent access
- For data that is less frequently accessed, but requires rapid access when needed 
- Lower cost than S3 standard
- S3 standard - infrequent Access (S3 standard IA)
	- 99.9% availability
	- Uses cases: Disaster recovery, backups
- S3 One zone - infrequent Access (S3 One Zone - IA)
  High durability (11 9's) in a single AZ; data lost when AZ is destroyed
	- 99.5 % availability
	- use cases: Storing secondary backup copies of an on-premise data, or data you can recreate

#### Glacier storage classes
- Low cost object storage meant for archiving/backup
- Pricing: price for storage + object retrieval cost
- Amazon S3 Glacier Instant Retrieval
	- Millisecond retrieval, great for data accessed once a quarter
	- Minimum storage duration of 90 days
- S3 Glacier Flexible Retrieval (formerly s3 glacier)
	- Expedited (1 to 5 minutes), Standard (3 to 5 hours) , bulk (5 to 12 hours) -free
	- Minimum storage duration of 90 days
- S3 Glacier Deep Archive - for long term storage
	- Standard (12 hrs), bulk (48 hrs)
	- Minimum storage duration of 180 days


#### S3 Intelligent - tiering
- Small monthly monitoring and auto-tiering fee
- moves object automatically between access Tiers based on usage
- There are no retrieval charges in S3 intelligent tiering
	- Frequent Access tier (automatic) default tier
	- Infrequent Access tier (automatic): objects not accessed for 30 days
	- Archive Instant Access Tier (automatic): objects not accessed from 90 days
	- Archive Access tier (optional): configurable from 90 days to 700+ days
	- Deep Archive Access tier (optional) config form 180 days to 700+ days

![[Pasted image 20250726132110.png]]

#### Moving between storage classes
- You can transition object between storage classes![[Pasted image 20250726171002.png]]
- For infrequently accessed object, move them to standard IA
- For archive objects that you don't need fast access to, move them to Glacier or Glacier Deep Archive
- Moving objects can be automated using a Lifecycle Rules

#### Lifecycle Rules
- Transition actions - configure objects to transition to another storage class
	- Move objects to standard IA class 60 days after creating
	- Move to Glacier for archiving after 6 months
- Expiration actions - configure objects to expire (delete) after some time
	- Access log files can be set to delete after a 365 days
	- Can be used to delete old version of files (if versioning is enabled)
	- Can be used to delete incomplete multi-part uploads
- Rules can be created for a certain prefix (example+: s3://mybucket/mp3/*)
- Rules can be created for certain objects Tags (example Department:Finance)
- Scenario
	- ![[Pasted image 20250726171326.png]]
	- ![[Pasted image 20250726171357.png]]


#### S3 Analytics - Storage class analysis
- Help you decide when to transition objects to the right storage class
- Recommendations for Standard and Standard IA
	- Does not work for one zone IA or Glacier
- Report is updated daily
- 24 to 48 hours to start seeing data analysis
- Good first step to put together LIfecycle rules
- ![[Pasted image 20250726171528.png]]

### S3 Requester Pays
- In general bucket owners pay for all S3 storage and data transfer cost associated with their bucket
- With Requester Pays bucket, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket
- Helpful when you want to share large datasets with other accounts
- The requester must be authenticated in AWS (cannot be anonymous) 
- ![[Pasted image 20250726171803.png]]

### Event Notifications
- S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication...
- Object name filtering possible (*.jpg)
- Use case: generate thumbnails of images uploaded to S3
- Can create as many s3 events as desired
- S3 event notifications typically deliver events in second but can sometimes take a minute or longer 
- ![[Pasted image 20250726171955.png]]
- IAM Permission
	- ![[Pasted image 20250726172041.png]]
- Amazon Event Bridge
	- ![[Pasted image 20250726172135.png]]
	- Advanced filtering options with JSON rules (metadata, object size, name)
	- Multiple destinations - Step Functions, Kinesis Streams / Firehose
	- EventBridge capabilities - Archive, Replay events, reliable delivery
### S3 Performance
- S3 automatically scales to high request rates, latency 100-200 ms
- Your application can achieve at least **3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket**
- There are no limits to the number of prefixes in a bucket
- Example (object path => prefix):
	- bucket/folder1/sub1/file => /folder1/sub1/
	- bucket/folder1/sub2/file => /folder1/sub2
	- bucket/1/file => /1/
	- bucket/2/file => /2/
- If you spread reads accross all four prefixes evenly, you can achieve 22000 requests per second for GET and HEAD
#### S3 Performance
- Multi-Part upload:
	- recommended for files > 100mb, must use for files >5GB
	- Can help parallelize uploads (speed up transfer)
	- ![[Pasted image 20250726173123.png]]
- S3 Transfer Acceleration
	- Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region
	- Compatible with multi-part upload
	- ![[Pasted image 20250726173204.png]]
- S3 Byte Range Fetches
	- Parallelize GETs by requesting specific byte ranges
	- Better resilience in case of failure
	- Can be used to speed up downloads
	- Can be used to retrieve only partial data (for example the head of a file)
	- ![[Pasted image 20250726173255.png]]

### S3 Batch Operations
- Perform bulk operations on existing S3 objects with a single request:
	- Modify object metadata and properties
	- Copy objects between s3 buckets
	- encrypt un-encrypted objects
	- modify ACLs,tags
	- restore objects from s3 glacier,
	- invoke lambda functions to perform custom action on each object
- A job consist of a list of objects, the action to perform, and optional parameters
- S3 Batch operations manages retries, tracks progress, sends completion notifications, generate reports...
- You can use S3 Inventory to get object list and use Athena to query and filter your objects
- ![[Pasted image 20250726173502.png]]
### S3 storage lens
- Understand, analyzed and optimize storage across entire AWS organization
- Discover anomalies, identify cost efficiencies, and apply data protection best practices across entire AWS Organization (30 days usage and activity metrics)
- Aggregrate data for Organization, specific accounts, regions, buckets, or prefixes
- Default dashboard or create your own dashboards
- can be configured to export metrics daily to an S3 bucket (CSV, Parquet)
- ![[Pasted image 20250726173652.png]]
#### Default Dashboard
- Visualize summarized insights and trends for both free and advanced metrics
- Default dashboard shows Multi-Region and Multi-Account data
- Preconfigured by Amazon S3
- can't be deleted, but can be disabled
- ![[Pasted image 20250726173744.png]]

#### Metrics
- Summary metrics
	- General insights about your S3 storage
	- StorageBytes,ObjectCount...
	- Use cases: identify the fastest growing (or not used) buckets and prefixes
- Cost-Optimization Metrics
	- Provide insights to manage and optimize your storage costs	
	- NonCurrentVersionStorageBytes,IncompleteMultiPartUploadStorageBytes
	- Use cases: indentify buckets with incomplete multipart uploaded older than 7 days, identify which objects could be transitioned to lower-cost storage class
- Data-protection Metrics
	- Provide insight for data protection features
	- Use cases: identify buckets that aren't following data-protection best practices
- Access-management Metrics
	- Provide insights for S3 Object Ownership
	- Use cases: identify which object ownership settings your buckets use
- Event Metrics:
	- Provide insights for S3 Event Notifications
- Performance Metrics
	- Provide insights for S3 TRansfer Acceleration
	- TransferAccelerationEnabledBucketCount (identify which buckets have S3 Transfer Acceleration enabled)
- Activity metrics
	- Provide insights about how your storage is requested
	- AllRequests,getRequests,PutRequests,....
- Detailed Status Code Metrics
	- Provide insights for HTTP status codes
	- 200OKStatusCount,404NotFoundErrorCount

#### Free vs Paid
- Free Metrics
	- Automatically available for all customers
	- Contains around 28 usage metrics
	- Data is available for for queries for 14 days
- Advanced Metrics and Recommendations
	- Additional paid metrics and features
	- Advanced Metrics - Activity, Advanced Cost Optimization, Advanced Data Protection,Status Code
	- CloudWatch Publishing - Access metrics in Cloudwatch without additional charges
	- Prefix Aggregation - Collect metrics at the prefix level
	- Data is available for queries for 15 months