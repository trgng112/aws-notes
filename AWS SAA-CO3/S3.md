- Main building blocks of AWS
- advertised as "infinite scaling" storage
- Many websites use S3 as a backbone
- Many AWS services use S3 as an integration as well

### Use cases
- Backup and storage
- Disaster recovery
- Archive
- Hybrid cloud storage
- Application hosting
- media hosting
- data lakes & big data analytics
- software delivery
- static website
- ![[Pasted image 20250726115747.png]]

### Buckets
- S3 allows people to store objects (files) in "buckets" (directories)
- buckets must have a globally unique name (across all regions all accounts)
- buckets are dined at the region levle
- **S3 looks like a global service but buckets are created in a region**
- Naming convention
	- No uppercase, no underscore
	- 3-63 characters long
	- Not an IP
	- Must start with lowercase letter or number
	- Must Not start with the prefix xn--
	- Must not end with the suffix -s3alias

### Objects
- Objects (files) have a Key
- the Key is the FULL path:
	- s3://my-bucket/**my_file.txt**
	- s3://my-bucket/**my_folder1/anothger_folder/my_file.txt**
- The key is composed of prefix + object name
	- s3://my-bucket/**my_folder1/another_folder/my_file.txt**
-  theres no concept of "directories" within buckets (although the UI will trick you to think otherwise)
- just keys with very long name that contains slashes
- Object values are the content of the body:
	- Max object size is 5TB
	- If uploading more than 5gb, must use "multi-part upload"
- Metadata (list of text key/value pairs - system or user metadata)
- Tags (unicode key/ value pair -up to 10) - useful for security / lifecycle
- Version ID (if versioning is enabled)


### Security
- User based
	- IAM policies - which API calls should be allowed for a specific user from IAM
- Resource-based
	- Bucket policies - bucket wide rules from the S3 console - allows cross account
	- Object Access Control LIst (ACL) - finer grain (can be disabled)
	- Bucket Access Control List (ACL) - less common (can be disabled)
- **Note: an IAM principal can access an S3 object if**
	- **The user IAM permissions ALLOW it OR the resource policy ALLOWS it**
	- **And There's no explicit DENY**
- Encryption: encrypt objects in Amazon S# using encryption keys

#### S3 Bucket Policies
- Json based policies
	- Resources: buckets and objects
	- Effect: Allow/ Deny
	- Actions: Set of API to Allow or Deny
	- Principal: The account or user to apply the policy to
- Use S3 bucket for policy to:
	- Grant public access to the bucket
	- Force objects to be encrypted at upload
	- Grant access to another account (Cross Account)
	- ![[Pasted image 20250726122705.png]]
- Example: 

	- ![[Pasted image 20250726122821.png]]
	- ![[Pasted image 20250726122831.png]]
	- ![[Pasted image 20250726122811.png]]
	- ![[Pasted image 20250726122910.png]]

### Static website hosting
- S3 can host static websites and have the accessible on the Internet 
- Website URL will be (depending on the region)
	- http://**bucket-name**.s3-websites-**aws-region**.amazonAWS.com
-  iF YOU GET 403 MAKE SURE THE BUKCET POLICY ALLOWS PUBLIC READS

### Versioning
- You can version your files in the S3
- enabled at the bucket level
- Same key overwrite will change the version: 1,2,3
- it is best practice to version your buckets
	- Protect against unintended deletes (restore a version)
	- easy rollback to previous version
- Notes:
	- Any file that is not versioned prior to enabling versioning will have version "null"
	- Suspending versioning does not delete the previous versions
### Replication (CRR & SRR)
- Must enable Versioning in source and destination buckets
- Cross- region replication (CRR)
- Same-Region Replication (SRR)
- Buckets can be different AWS accounts
- Copying is asynchronous
- Must give proper IAM permissions to S3
- Use cases:
	- CRR - compliance, lower latency access, replication across accounts
	- SRR - log aggregation, live replication between production and test accounts
- Notes:
	- After you enable replication, only new objects are replicated
	- optionally, you can replicate existing objects using **S3 Batch Replication**
		- Replicates existing objects and objects that failed replication
	- For DELETE operations
		- **Can replicate delete markers** from source to target (optional settings)
		- Deletions with a version ID are not replicated (to avoid malicious deletes)
	- **There is no chaining of replication** 
		- If bucket 1 has replication into bucket 2, which has replication into bucket 3
		- Then objects created in bucket 1 are not replicated to bucket 3

### S3 Storage Classes
- S3 Standard - General purpose
- Standard - Infrequent access (IA)
- One ZOne - Infrequent ACcess
- Glacier Instant Retrieval
- Glacier Flexible Retrieval
- Glacier Deep Archive
- Intelligent tiering
- Can move between classes manually or using S3 lifecycle configurations

#### Durability and availability
- Durability:
	- High durability (99.99999999999%, 11 9's) of objects across multiple AZ
	- if you store 10,000,000 objects with S3, you can on average expect to incur a loss of a single object once every 10,000 years
	- Same for all storage classes
- Availability:
	- Measures how readily available a service is
	- Varies depending on storage class
	- example: s3 standard has 99.99% availability = not available 53 minutes a year

#### S3 Standard - General purpose
- 99.99 availability
- used for frequently accessed data
- low latency and high throughput
- sustain 2 concurrent facility failures
- use cases: big data analytics, mobile, gaming applications, content distribution

#### Infrequent access
- For data that is less frequently accessed, but requires rapid access when needed 
- Lower cost than S3 standard
- S3 standard - infrequent Access (S3 standard IA)
	- 99.9% availability
	- Uses cases: Disaster recovery, backups
- S3 One zone - infrequent Access (S3 One Zone - IA)
  High durability (11 9's) in a single AZ; data lost when AZ is destroyed
	- 99.5 % availability
	- use cases: Storing secondary backup copies of an on-premise data, or data you can recreate

#### Glacier storage classes
- Low cost object storage meant for archiving/backup
- Pricing: price for storage + object retrieval cost
- Amazon S3 Glacier Instant Retrieval
	- Millisecond retrieval, great for data accessed once a quarter
	- Minimum storage duration of 90 days
- S3 Glacier Flexible Retrieval (formerly s3 glacier)
	- Expedited (1 to 5 minutes), Standard (3 to 5 hours) , bulk (5 to 12 hours) -free
	- Minimum storage duration of 90 days
- S3 Glacier Deep Archive - for long term storage
	- Standard (12 hrs), bulk (48 hrs)
	- Minimum storage duration of 180 days


#### S3 Intelligent - tiering
- Small monthly monitoring and auto-tiering fee
- moves object automatically between access Tiers based on usage
- There are no retrieval charges in S3 intelligent tiering
	- Frequent Access tier (automatic) default tier
	- Infrequent Access tier (automatic): objects not accessed for 30 days
	- Archive Instant Access Tier (automatic): objects not accessed from 90 days
	- Archive Access tier (optional): configurable from 90 days to 700+ days
	- Deep Archive Access tier (optional) config form 180 days to 700+ days

![[Pasted image 20250726132110.png]]

#### Moving between storage classes
- You can transition object between storage classes![[Pasted image 20250726171002.png]]
- For infrequently accessed object, move them to standard IA
- For archive objects that you don't need fast access to, move them to Glacier or Glacier Deep Archive
- Moving objects can be automated using a Lifecycle Rules

#### Lifecycle Rules
- Transition actions - configure objects to transition to another storage class
	- Move objects to standard IA class 60 days after creating
	- Move to Glacier for archiving after 6 months
- Expiration actions - configure objects to expire (delete) after some time
	- Access log files can be set to delete after a 365 days
	- Can be used to delete old version of files (if versioning is enabled)
	- Can be used to delete incomplete multi-part uploads
- Rules can be created for a certain prefix (example+: s3://mybucket/mp3/*)
- Rules can be created for certain objects Tags (example Department:Finance)
- Scenario
	- ![[Pasted image 20250726171326.png]]
	- ![[Pasted image 20250726171357.png]]


#### S3 Analytics - Storage class analysis
- Help you decide when to transition objects to the right storage class
- Recommendations for Standard and Standard IA
	- Does not work for one zone IA or Glacier
- Report is updated daily
- 24 to 48 hours to start seeing data analysis
- Good first step to put together LIfecycle rules
- ![[Pasted image 20250726171528.png]]

### S3 Requester Pays
- In general bucket owners pay for all S3 storage and data transfer cost associated with their bucket
- With Requester Pays bucket, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket
- Helpful when you want to share large datasets with other accounts
- The requester must be authenticated in AWS (cannot be anonymous) 
- ![[Pasted image 20250726171803.png]]

### Event Notifications
- S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication...
- Object name filtering possible (*.jpg)
- Use case: generate thumbnails of images uploaded to S3
- Can create as many s3 events as desired
- S3 event notifications typically deliver events in second but can sometimes take a minute or longer 
- ![[Pasted image 20250726171955.png]]
- IAM Permission
	- ![[Pasted image 20250726172041.png]]
- Amazon Event Bridge
	- ![[Pasted image 20250726172135.png]]
	- Advanced filtering options with JSON rules (metadata, object size, name)
	- Multiple destinations - Step Functions, Kinesis Streams / Firehose
	- EventBridge capabilities - Archive, Replay events, reliable delivery
### S3 Performance
- S3 automatically scales to high request rates, latency 100-200 ms
- Your application can achieve at least **3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket**
- There are no limits to the number of prefixes in a bucket
- Example (object path => prefix):
	- bucket/folder1/sub1/file => /folder1/sub1/
	- bucket/folder1/sub2/file => /folder1/sub2
	- bucket/1/file => /1/
	- bucket/2/file => /2/
- If you spread reads accross all four prefixes evenly, you can achieve 22000 requests per second for GET and HEAD
#### S3 Performance
- Multi-Part upload:
	- recommended for files > 100mb, must use for files >5GB
	- Can help parallelize uploads (speed up transfer)
	- ![[Pasted image 20250726173123.png]]
- S3 Transfer Acceleration
	- Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region
	- Compatible with multi-part upload
	- ![[Pasted image 20250726173204.png]]
- S3 Byte Range Fetches
	- Parallelize GETs by requesting specific byte ranges
	- Better resilience in case of failure
	- Can be used to speed up downloads
	- Can be used to retrieve only partial data (for example the head of a file)
	- ![[Pasted image 20250726173255.png]]

### S3 Batch Operations
- Perform bulk operations on existing S3 objects with a single request:
	- Modify object metadata and properties
	- Copy objects between s3 buckets
	- encrypt un-encrypted objects
	- modify ACLs,tags
	- restore objects from s3 glacier,
	- invoke lambda functions to perform custom action on each object
- A job consist of a list of objects, the action to perform, and optional parameters
- S3 Batch operations manages retries, tracks progress, sends completion notifications, generate reports...
- You can use S3 Inventory to get object list and use Athena to query and filter your objects
- ![[Pasted image 20250726173502.png]]
### S3 storage lens
- Understand, analyzed and optimize storage across entire AWS organization
- Discover anomalies, identify cost efficiencies, and apply data protection best practices across entire AWS Organization (30 days usage and activity metrics)
- Aggregrate data for Organization, specific accounts, regions, buckets, or prefixes
- Default dashboard or create your own dashboards
- can be configured to export metrics daily to an S3 bucket (CSV, Parquet)
- ![[Pasted image 20250726173652.png]]
#### Default Dashboard
- Visualize summarized insights and trends for both free and advanced metrics
- Default dashboard shows Multi-Region and Multi-Account data
- Preconfigured by Amazon S3
- can't be deleted, but can be disabled
- ![[Pasted image 20250726173744.png]]

#### Metrics
- Summary metrics
	- General insights about your S3 storage
	- StorageBytes,ObjectCount...
	- Use cases: identify the fastest growing (or not used) buckets and prefixes
- Cost-Optimization Metrics
	- Provide insights to manage and optimize your storage costs	
	- NonCurrentVersionStorageBytes,IncompleteMultiPartUploadStorageBytes
	- Use cases: indentify buckets with incomplete multipart uploaded older than 7 days, identify which objects could be transitioned to lower-cost storage class
- Data-protection Metrics
	- Provide insight for data protection features
	- Use cases: identify buckets that aren't following data-protection best practices
- Access-management Metrics
	- Provide insights for S3 Object Ownership
	- Use cases: identify which object ownership settings your buckets use
- Event Metrics:
	- Provide insights for S3 Event Notifications
- Performance Metrics
	- Provide insights for S3 TRansfer Acceleration
	- TransferAccelerationEnabledBucketCount (identify which buckets have S3 Transfer Acceleration enabled)
- Activity metrics
	- Provide insights about how your storage is requested
	- AllRequests,getRequests,PutRequests,....
- Detailed Status Code Metrics
	- Provide insights for HTTP status codes
	- 200OKStatusCount,404NotFoundErrorCount

#### Free vs Paid
- Free Metrics
	- Automatically available for all customers
	- Contains around 28 usage metrics
	- Data is available for for queries for 14 days
- Advanced Metrics and Recommendations
	- Additional paid metrics and features
	- Advanced Metrics - Activity, Advanced Cost Optimization, Advanced Data Protection,Status Code
	- CloudWatch Publishing - Access metrics in Cloudwatch without additional charges
	- Prefix Aggregation - Collect metrics at the prefix level
	- Data is available for queries for 15 months


### S3 Security
#### Object Encryption
- You can encrypt object in S3 buckets using one of 4 methods
- **Server side Encryption (SSE)**
	- Server side encryption with S3 managed keys (SSE-S3) - enabled by default
	- Encryption using keys handle,managed,and owned by AWS
	- Object is encrypted server--side
	- Encryption type is **AES-256**
	- Must set header "x-amz-server-side-encryption":"AEW256"
	- Enabled by default for new buckets and new objects
	- ![[Pasted image 20250727124501.png]]
- Server-side encryption with KMS keys stored in AWS KMS (SSE-KMS)
	- Leverages AWS KMS to manage encryption keys
	- Encryption using keys handled and manage by AWS KMS (Key Management Service)
	- KMS advantages: user control + audit key usage using CloudTrail
	- Object is encrypted server side
	- must set header **"x-amz-server-side-encryption":"aws:kms"
	- ![[Pasted image 20250727124704.png]]
	- Limitation:
		- If you use SSE KMS , you may be impacted by the KMS limits,
		- when you upload, it calls the **GenerateDataKey** KMS API
		- When you download, it calls the **Decrypt** KMS API
		- Count towards the KMS quota per second (5500,10000,30000 req/s based on region)
		- You can request a quota increase using the Service Quotas Console
		- ![[Pasted image 20250727124901.png]]
- Server-side Encryption with Customer Provided Keys (SSE-C)
	- When you want to manage your own encryption keys
	- using keys fully managed by the customer outside of AWS
	- S3 does NOT store the encryption key you provide
	- **HTTPS must be used**
	- Encryption key must provided in HTTP headers, for every HTTP request made
	- ![[Pasted image 20250727124946.png]]
	- 
- **Client-side encryption**
	- Use client libraries such as **Amazon S3 Client-side Encryption Library**
	- Clients must encrypt data themselves before sending to amazon s3
	- Clients must decrypt data  themselves when retrieving from S3
	- Customer fully manages the keys and encryption cycle
	- ![[Pasted image 20250727125112.png]]
- Encryption in transit (SSL/TLS)
	- Encryption in flight is also called SSL/TLS
	- S3 exposes 2 endpoints
		- HTTP Endpoint - non encrypted
		- HTTPS endpoint - encryption in flight
	- HTTPS is recommended
	- HTTPS is mandatory for SSE-C
- It's important to understand which ones are for which situation for the exam


#### Default Encryption vs Bucket Policies
- SSE-S3 encryption is automatically applied to new objects stored in S3 bucket
- Optionally, you can "force encryption" using a bucket policy and refuse any API call to PUT an S3 object without encryption headers (SSE-KMS or SSE-C)
- ![[Pasted image 20250727125844.png]]
- Note: Bucket policies are evaluated before "Default Encryption"



### CORS
- Cross-Origin Resource Sharing
- Origin = scheme (protocol) + host(domain) +port
	- example: https://www.example.com (implied port is 443 for HTTPS, 80 for HTTP)
- Web browser based mechanism to allow requests to other origins while visiting the main origin
- same origin: http://example.com/app1 & http://example.com/app2
- Different origins: http://www.example.com & http://other.example.com
- The requests won't be fulfilled unless the other origin allows for the requests, using **CORS Headers** (example: **Access-Control-Allow-Origin**)
-  ![[Pasted image 20250727130302.png]]
- S3 - CORS
	- If a client makes a cross-origin request on our S3 bucket, we need to enabled the correct CORS headers
	- It's a popular exam question
	- You can allow for a specific origin or for * (all origins)
	- ![[Pasted image 20250727130435.png]]
	- 


### S3 Access Logs
- For audit purpose, you may want to log all access to S3 buckets
- Any requests made to S3, from any account, authorized or denied, will be logged into another S3 bucket
- That data can be analyzed using data analysis tools....
- The target logging bucket must be in the same AWS region
- warning:
	- Do not set your logging bucket to be the monitored bucket
	- It will create a logging loop, and **your bucket will grow exponentially**
	- ![[Pasted image 20250727134432.png]]

### Pre signed URLs
- Generate pre-signed URLs using the S3 Console, AWS CLI or SDK
- URL Expiration
	- S3 console - 1 min up to 720 mins (12 hrs)
	- AWS CLI - configure expiration with -expires-in parameter in seconds 
- Users give a pre-signed URL inherit the permissions of tghe user that generated the URL for GET/PUT
- ![[Pasted image 20250727134801.png]]
- Examples:
	- Allow only logged-in users to download a premium video from your s3 bucket
	- Allow an ever changing list of users to download files by generating URLs dynamically
	- Allow temporarily a user to upload a file to a precise location in your s3 bucket

### S3 Glacier Vault Lock and Object Lock
#### Glacier Vault Lock
- Adopt a WORM (Write Once Read Many) model
- Create a Vault Lock Policy
- Lock the policy for future edits (can no longer be changed or deleted)
- Helpful for compliance and data retention
- ![[Pasted image 20250727135056.png]]

#### S3 Object Lock
- Adopt WORM model
- Block an object version deletion for a specified amount of time
- **Retention mode - Compliance**
	- Object versions can't be overwritten or deleted by any user, including the root user
	- Objects retention modes can't be changed, and retention periods can't be shortened
- **Governance**
	- Most users can't overwrite or delete an object version or alter its lock settings
	- Some users have special permissions to change the retention or delete the object
- Retention period: protect the object for a fixed period, it can be extended
- Legal Hold:
	- Protect the object indefinitely, independent from retention period
	- can be freely placed and remove using s3:PutObjectLegalHold IAM permission



### Access Points
- Simplify security management for S3 Buckets
- Each Access Point has:
	- its own DNS name (internet Origin or VPC origin)
	- an access point policy (similar to bucket policy) - manage security at scale
- ![[Pasted image 20250727135551.png]]
- VPC origin
	- We can define the access point to be accessible only from within the VPC
	- You must create a VPC Endpoint to access the Access Point (gateway or interface endpoint)
	- The VPC Endpoint Policy must allow access to the target bucket and Access Point
	- ![[Pasted image 20250727135659.png]]


### S3 object Lambda
- Use AWS Lambda Functions to change the object before it is retrieved the the caller application
- only one S3 bucket is needed ,on top of which we create **S3 Access Point** and **S3 Object Lambda Access Points**
- Use cases:
	- Redacting personally identifiable information for analytics or non-production environments
	- converting across data formats such as XML to JSON
	- Resizing and watermarking images on the fly using caller-specific details, such as the user who requested the object
	- ![[Pasted image 20250727135956.png]]
	- 